---
title: "Estimate marginal effects"
output:
  rmarkdown::html_vignette:
    toc: true
    fig_width: 10.08
    fig_height: 6
tags: [r, estimate, estimate slopes, marginal effects]
vignette: >
  %\VignetteIndexEntry{Estimate marginal effects}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  chunk_output_type: console
bibliography: bibliography.bib
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
options(knitr.kable.NA = "")
knitr::opts_chunk$set(
  comment = ">",
  dpi = 300,
  message = FALSE,
  warning = FALSE
)
options(digits = 2)

if (!requireNamespace("ggplot2", quietly = TRUE) ||
  !requireNamespace("see", quietly = TRUE) ||
  !requireNamespace("gganimate", quietly = TRUE) ||
  !requireNamespace("emmeans", quietly = TRUE) ||
  !requireNamespace("marginaleffects", quietly = TRUE) ||
  !requireNamespace("rstanarm", quietly = TRUE)) {
  knitr::opts_chunk$set(eval = FALSE)
}

set.seed(333)
```

This vignette will present how to estimate marginal effects and derivatives using
`estimate_slopes()`.

**Marginal means *vs.* Marginal effects**. Marginal slopes are to numeric predictors what marginal means are to categorical predictors, in the sense that they can eventually be "averaged over" other predictors of the model. The key difference is that, while marginal means return averages of the outcome variable, which allows you to say for instance "the average reaction time in the C1 condition is 1366 ms", marginal effects return **averages of coefficients**. This allows you to say, for instance, "the average *effect of x* in the C1 condition is 0.33". When you visualize marginal effects, the y-axis is the effect/slope/beta of a given numeric predictor.

Let's see some examples.

## Marginal effects over a factor's levels

Let's fit a linear model with a factor interacting with a continuous predictor and visualize it.

```{r}
library(ggplot2)
library(parameters)
library(performance)
library(modelbased)

model <- lm(Sepal.Length ~ Petal.Length * Species, data = iris)

estimate_relation(model) |>
  plot()
```

It seems like the slope of the effect is roughly similar (in the same direction) across the different factor levels.

```{r}
parameters(model)
```

Moreover, the interaction is not significant. However, we see below that removing the interaction does not *substantially* improve the model's performance. So, for the sake of the demonstration, let's say we want to keep the maximal effect structure.

```{r}
model2 <- lm(Sepal.Length ~ Petal.Length + Species, data = iris)

test_performance(model, model2)
```

Although we are satisfied with our model and its performance, imagine we are not interested in the effect of `Petal.Length` for different Species, but rather, in its **general trend** "across" all different species. We need to compute the **marginal effect** of the predictor, which corresponds to its slope *averaged* (it's a bit more complex than a simple averaging but that's the idea) over the different factor levels.

```{r}
slopes <- estimate_slopes(model, trend = "Petal.Length")

slopes
```

We can see that the effect of `Petal.Length`, **marginalized over Species**, is positive and significant.

## Effects for each factor's levels


```{r}
slopes <- estimate_slopes(model, trend = "Petal.Length", by = "Species")

slopes

plot(slopes)
```


## Interactions between two continuous variables

Interactions between two continuous variables are often not straightforward to visualize and interpret. Thanks to the model-based approach, one can represent the effect of one of the variables as a function of the other variable.

Such plot, also referred to as **Johnson-Neyman intervals**, shows how the effect (the "slope") of one variable varies depending on another variable. It is useful in the case of complex interactions between continuous variables. See also [this vignette](https://easystats.github.io/modelbased/articles/introduction_comparisons_3.html) for further details.

For instance, the plot below shows that the effect of `hp` (the y-axis) is significantly negative only when `wt` is low (`< ~4`).

```{r}
model <- lm(mpg ~ hp * wt, data = mtcars)

slopes <- estimate_slopes(model, trend = "hp", by = "wt")

plot(slopes) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  theme_minimal()
```

## Describing and reporting non-linear relationships (e.g., in GAMs)

**Complex problems require modern solutions.**

General Additive Models (GAMs) are a powerful class of models that extend the capabilities of traditional GLMs. In particular, they are able to parsimoniously model possibly non-linear relationship.

Let's take for instance the following model:

```{r}
# Fit a non-linear General Additive Model (GAM)
model <- mgcv::gam(Sepal.Width ~ s(Petal.Length), data = iris)

estimate_relation(model, length = 50) |>
  plot()
```

The GAMs nicely models the complex relationship between the two variables (if we don't take into account the different species of course).

But how to interpret and report in a manuscript such results? We can't simply paste the figure right? Right? Reviewers will want some statistics, some numbers between brackets, otherwise it doesn't look serious does it.

```{r}
parameters::parameters(model)
```


The problem with GAMs is that their parameters (i.e., the coefficients), are not easily interpretable. Here we can see one line corresponding to the smooth term. It's significant, great, but what does it mean? And if we run the GAM using other packages (e.g., `{rstanarm}` or `{brms}`), the parameters will not be the same. What to do!

Because the meaning of these parameters is somewhat disconnected with our need for relationship understanding, another possibility is to compute the **marginal linear effect** of the smooth term, i.e., the "derivative", using `estimate_slopes`.


```{r}
# Compute derivative
deriv <- estimate_slopes(model,
  trend = "Petal.Length",
  by = "Petal.Length",
  length = 100
)

# Visualise
plot(deriv) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  theme_minimal()
```

This plot represents the "slope" of the curve at each point of the curve. As you can see, there is a significant negative trend (after Petal.Length = 2), followed by a significant positive trend (around Petal.Length = 4). **Marginal derivatives** allow us to make inferences at each point of the relationship!

Finally, to help reporting that in a manuscript, we can divide this into chunks and obtain a summary for the trend of each chunk, including the direction of the effects and whether it is statistically significant or not.

```{r}
summary(deriv)
```

## References

Johnson, P.O. & Fay, L.C. (1950). The Johnson-Neyman technique, its theory and application. Psychometrika, 15, 349-367. doi: 10.1007/BF02288864
